<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Neural Networks from Scratch | Prathamesh Devadiga</title> <meta name="author" content="Prathamesh Devadiga"> <meta name="description" content="Talk given in PES University, ECC."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://devadigapratham.github.io/talks/NNFS/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Prathamesh </span>Devadiga</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/catalysis/">catalysis</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Neural Networks from Scratch</h1> <p class="post-description">Talk given in PES University, ECC.</p> </header> <article> <p>Hello everyone, so in this blog, I will be talking about <code class="language-plaintext highlighter-rouge">building</code> neural networks from <code class="language-plaintext highlighter-rouge">scratch</code>. ps : this was my first talk.</p> <h2 id="introduction--">Introduction : <br> </h2> <p>So, What are Neural Networks? These are a set of algorithms modeled loosely after the human brain usually designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.</p> <h2 id="fundamental-concepts-you-should-know-">Fundamental Concepts You Should Know :</h2> <p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Artificial-Intelligence-Neural-Network-Nodes.jpg" alt="nn" width="700" height="400"><br></p> <p>Before you begin building your own Neural Network, here are some key concepts that you should be having a good understanding of:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Layers</code> : These are the building blocks of neural networks, with each layer transforming its input data to a more abstract level.</li> <li> <code class="language-plaintext highlighter-rouge">Neurons</code>: The fundamental processing units within layers that apply weights to their inputs and pass the result through an activation function.</li> <li> <code class="language-plaintext highlighter-rouge">Weights and Biases</code>: Parameters within neurons that are tuned during the training process to minimize the network’s error.</li> <li> <code class="language-plaintext highlighter-rouge">Perceptron</code>: This is the simplest type of an artificial neural network. It is a simple binary classifier that maps the input (which is a real-valued vector) to an output value (integer).</li> <li> <code class="language-plaintext highlighter-rouge">Activation Function</code>: This function decides whether a neuron should be activated or not. It takes a weighted sum of the inputs as an argument and returns a value between 0 and 1. There are multiple types of activation functions which could include ReLU, Sigmoid, tanH, etc.</li> <li> <code class="language-plaintext highlighter-rouge">Loss Function</code>: This measures the difference between the predicted and actual values. The goal of training is to minimize this function.</li> <li> <code class="language-plaintext highlighter-rouge">Optimization Algorithm</code>: This updates the parameters of the model in order to minimize the loss function.</li> <li> <code class="language-plaintext highlighter-rouge">Backpropagation</code>: This is the method used to update the weights of the network. It calculates the gradient of the loss function with respect to the weights and updates them accordingly.</li> <li> <code class="language-plaintext highlighter-rouge">Optimizers</code>: Algorithms that adjust the weights and biases to minimize the loss function.</li> </ul> <h2 id="what-is-synthetic-spiral-data">What is Synthetic Spiral Data?</h2> <p>Synthetic spiral data is a set of two-dimensional data points arranged in concentric circles, with each circle representing a different class. Each point within a circle is assigned to that circle’s class. This type of data is often used in machine learning and data science tutorials because it’s relatively easy to generate and can illustrate complex concepts.</p> <h2 id="how-to-build-your-own-neural-network">How to Build Your Own Neural Network?</h2> <p><img src="https://miro.medium.com/v2/resize:fit:500/0*eaU-biq79qgFjned.jpg" alt="nn" width="500" height="550"><br></p> <p>Coming to the exciting part of this blog, hehe. You can follow this procedure to build your own neural network :</p> <ol> <li> <code class="language-plaintext highlighter-rouge">Designing the Network Architecture</code>: Decide on the number of layers, neurons per layer, and activation functions.</li> <li> <code class="language-plaintext highlighter-rouge">Initializing Parameters</code>: Randomly initialize weights and biases.</li> <li> <code class="language-plaintext highlighter-rouge">Forward Propagation</code>: Compute the network’s output for a given input.</li> <li> <code class="language-plaintext highlighter-rouge">Calculating Loss</code>: Measure how far the network’s predictions are from the actual values.</li> <li> <code class="language-plaintext highlighter-rouge">Backpropagation</code>: Adjust the weights and biases in the direction that reduces loss.</li> <li> <code class="language-plaintext highlighter-rouge">Iterating</code>: Repeat the forward pass, loss calculation, and backpropagation for multiple epochs, using a dataset to train the network.</li> </ol> <h2 id="code-examples-">Code Examples :</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">Layer_Dense</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="mf">0.10</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">biases</span>

<span class="k">class</span> <span class="nc">Activation_ReLU</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Activation_SoftMax</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">exp_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">exp_values</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">probabilities</span>

<span class="k">class</span> <span class="nc">Loss_CategoricalCrossEntropy</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">):</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">y_pred_clipped</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-7</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">correct_confidences</span> <span class="o">=</span> <span class="n">y_pred_clipped</span><span class="p">[</span><span class="nf">range</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">y_true</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">correct_confidences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_pred_clipped</span> <span class="o">*</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">neg_log_likelihoods</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">correct_confidences</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">neg_log_likelihoods</span>

<span class="k">class</span> <span class="nc">Optimizer_SGD</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

    <span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
        <span class="n">layer</span><span class="p">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">layer</span><span class="p">.</span><span class="n">dweights</span>
        <span class="n">layer</span><span class="p">.</span><span class="n">biases</span> <span class="o">+=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">layer</span><span class="p">.</span><span class="n">dbiases</span>

</code></pre></div></div> <p>In this code, we have defined several classes to represent different components of a neural network:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Layer_Dense</code>: This class represents a fully connected layer of the neural network. It has a method forward for forward propagation.</li> <li> <code class="language-plaintext highlighter-rouge">Activation_ReLU and Activation_SoftMax</code>: These classes represent the activation functions used in the neural network. They have a method forward for forward propagation.</li> <li> <code class="language-plaintext highlighter-rouge">Loss_CategoricalCrossEntropy</code>: This class represents the loss function used to measure the difference between the predicted and actual outputs. It has a method forward for forward propagation.</li> <li> <code class="language-plaintext highlighter-rouge">Optimizer_SGD</code>: This class represents the optimizer used to update the weights and biases of the neural network. It has a method update_params for updating the parameters.</li> </ul> <h2 id="techniques-to-improve-accuracy-">Techniques to Improve Accuracy :</h2> <p>You could use the following strategies to improve the accuracy :</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Increase the Number of Layers</code>: More layers can lead to better feature extraction and improved accuracy.</li> <li> <code class="language-plaintext highlighter-rouge">Increase the Number of Neurons</code>: More neurons can provide a more complex and capable model.</li> <li> <code class="language-plaintext highlighter-rouge">Increase the Number of Epochs</code>: More epochs allow the model to learn from the data more thoroughly.</li> <li> <code class="language-plaintext highlighter-rouge">Data Augmentation</code>: Generating new training samples by applying transformations to existing data can improve generalization.</li> <li> <code class="language-plaintext highlighter-rouge">Hyperparameter Tuning</code>: Adjusting learning rate, batch size, and other parameters can lead to better performance.</li> <li> <code class="language-plaintext highlighter-rouge">Regularization Techniques</code>: Methods like dropout can prevent overfitting and improve model generalization.</li> <li> <code class="language-plaintext highlighter-rouge">Transfer Learning</code>: Using pre-trained models as a starting point can accelerate learning and lead to higher accuracy.</li> </ul> <h2 id="conclusion-">Conclusion :</h2> <p><img src="https://programmerhumor.io/wp-content/uploads/2023/10/programmerhumor-io-linux-memes-python-memes-0a1699c1209a26a-608x600.jpg" alt="nn" width="500" height="450"><br></p> <p>So, we’ve taken a deep dive into the fascinating world of neural networks figuring out how they work. Now, it’s your time to have some fun with it! Grab your coffee, get to biz and let your curiosity guide you as you explore and build. Enjoy the journey into Neural Networks!</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Prathamesh Devadiga. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>